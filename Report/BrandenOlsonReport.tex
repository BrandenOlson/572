\documentclass{uwstat572}

%%\setlength{\oddsidemargin}{0.25in}
%%\setlength{\textwidth}{6in}
%%\setlength{\topmargin}{0.5in}
%%\setlength{\textheight}{9in}

\renewcommand{\baselinestretch}{1.5} 

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{array}
\usepackage{bbm}
\usepackage{cancel}
\usepackage{courier}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{ mathrsfs }
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{setspace}

\pagenumbering{gobble}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}

\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\mean[1]{\overline{#1}}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\card[1]{\text{card}\left(#1\right)}
\newcommand*\D{\mathscr{D}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\;\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\;\mathsf{Var}\left(#1\right)}
\newcommand*\SD[1]{\;\mathsf{SD}\left(#1\right)}
\newcommand*\SE[1]{\;\mathsf{SE}\left(#1\right)}
\newcommand*\Cov[1]{\;\mathsf{Cov}\left(#1\right)}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[1]{\mathbbm{1}\left(#1\right)}
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\deriv[2]{\frac{\der #1}{\der #2}}
\newcommand*\pderiv[2]{\frac{\pd #1}{\pd #2}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}
\newcommand*\bolda{\mathbf{a}}
\newcommand*\bA{\mathbf{A}}
\newcommand*\bb{\mathbf{b}}
\newcommand*\bB{\mathbf{B}}
\newcommand*\bc{\mathbf{c}}
\newcommand*\bD{\mathbf{D}}
\newcommand*\be{\mathbf{e}}
\newcommand*\bG{\mathbf{G}}
\newcommand*\bh{\mathbf{h}}
\newcommand*\bI{\mathbf{I}}
\newcommand*\bell{\boldsymbol{\ell}}
\newcommand*\bL{\mathbf{L}}
\newcommand*\bN{\mathbf{N}}
\newcommand*\bo{\mathbf{o}}
\newcommand*\bp{\mathbf{p}}
\newcommand*\br{\mathbf{r}}
\newcommand*\bR{\mathbf{R}}
\newcommand*\bs{\mathbf{s}}
\newcommand*\bS{\mathbf{S}}
\newcommand*\bt{\mathbf{t}}
\newcommand*\bT{\mathbf{T}}
\newcommand*\bu{\mathbf{u}}
\newcommand*\bv{\mathbf{v}}
\newcommand*\bV{\mathbf{V}}
\newcommand*\bx{\mathbf{x}}
\newcommand*\bw{\mathbf{w}}
\newcommand*\bW{\mathbf{W}}
\newcommand*\bX{\mathbf{X}}
\newcommand*\by{\mathbf{y}}
\newcommand*\bY{\mathbf{Y}}
\newcommand*\bZ{\mathbf{Z}}
\newcommand*\bz{\mathbf{z}}
\newcommand*\bzero{\mathbf{0}}
\newcommand*\bone{\mathbf{1}}
\newcommand*\balpha{\boldsymbol{\alpha}}
\newcommand*\bbeta{\boldsymbol{\beta}}
\newcommand*\bgamma{\boldsymbol{\gamma}}
\newcommand*\bdelta{\boldsymbol{\delta}}
\newcommand*\bDelta{\boldsymbol{\Delta}}
\newcommand*\beps{\boldsymbol{\varepsilon}}
\newcommand*\btheta{\boldsymbol{\theta}}
\newcommand*\bTheta{\boldsymbol{\Theta}}
\newcommand*\bmu{\boldsymbol{\mu}}
\newcommand*\bpsi{\boldsymbol{\psi}}
\newcommand*\bsigma{\boldsymbol{\sigma}}
\newcommand*\bSigma{\boldsymbol{\Sigma}}
\newcommand*\btau{\boldsymbol{\tau}}
\newcommand*\bOmega{\boldsymbol{\Omega}}
\newcommand\Psub[2]{\mathsf{Pr}_{#1}\left(#2\right)}
\newcommand\Vsub[2]{\mathsf{Var}_{#1}\left(#2\right)}
\newcommand\e{\operatorname{e}}
\newcommand\prox{\operatorname{prox}}
\newcommand\T{\mathsf{T}}

\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right) }


\bibliographystyle{plainnat}

\begin{document}
%%\maketitle

\begin{center}
  {\LARGE A Report On ``Combining Mixture Components for Clustering'' by Baudry et al.}\\\ \\
  {Branden Olson\\ 
    Department of Statistics, University of Washington Seattle, WA, 98195, USA
  }
\end{center}



\begin{abstract}
Model-based clustering typically assumes an underlying Gaussian mixture model, but often the number of mixture components is unknown. 
The familiar Bayesian information criterion, which excels at inferring the number of components, frequently overestimates the number of clusters. 
An extension known as the integrated complete likelihood criterion, which penalizes model entropy, assuages this to yield a more accurate cluster count, although falls flat for non-Gaussian cluster distributions. 
\citet{Baudry10} propose a method with combines the strengths of the two approaches, yielding a sequence of clusterings starting from the BIC solution, and iteratively merging clusters to minimize the resultant entropy.
This method is applied to simulated and real data, demonstrating superior performance in each case.
\end{abstract}

\section{Introduction}

\subsection{The problem of clustering}
Many situations arise in which collected data points must be clustered into similar subgroups. 
For example, we might wish to cluster DNA sequences from a population into phylogenetic families, or categorize X-ray data readings by the galaxies from which the waves originated.
In general, we consider clusters to be contiguous, densely-populated regions of a feature space, separated by contiguous, relatively empty regions.
The problem of clustering can be quite challenging as the usual difficulties of modeling the underlying distribution are compounded by the absence of information about the nature of the clusters.
Indeed, the field of clustering contains a broad and vast set of approaches based on various assumptions of the data-generating mechanism and cluster specifications.

Clustering methods are motivated by heuristics, formal statistical models, or a combination thereof.
Hierarchical clustering techniques either merge or split clusters at each stage \citep{Fraley98}.
Agglomerative, or ``bottom-up'', clustering starts with a large number of clusters (often with each point comprising its own cluster) and merges the clusters based on some criterion.
These criteria include neigherest-neighbor, farthest-neighbor, and sum of squares, or maximum likelihood rules \citep{Kaufman90, Murtagh84, Banfield93}.
Divisive, or ``top-down'', clustering recursively divides the clusters starting from large groups. 
Another class of clustering tools known as relocation methods start from an initial division and successively move points to other groups.
In particular, the popular $k$-means relocation method reassigns points to reduce the intra-cluster sum of squares \citep{Hartigan78}. 

\subsection{Model-based clustering}

Another approach known as model-based clustering assumes that the data were generated from a mixture model of probability distributions and can be assigned cluster labels based on their most likely mixture components.
Here, we decompose the probability density $f$ of each point $\bx_i$ as a sum of scaled component densities parametrized by $\btheta$:
\ba
f(\bx_i ; K, \btheta_K)
	& = \sum_{k=1}^K p_k \phi(\bx_i ; \btheta).
\ea
Here, $p_k, k = 1, \dotsc, K$ are positive mixture proportions which sum to one, and $\phi$ is any valid density.
Modelers commonly choose Gaussian component densities, often with success \citep{Murtagh84, Banfield93, Dasgupta98, Campbell97, Celeux95, Mukerjee98}.
However, other distributions may better suit the nature of the data, making the choice of density context-dependent.
For example, the $t$-density is appropriate in the presence of outliers which heavily influence cluster definitions \citep{Peel00}.
The parameters of the underlying densities can be estimated using the expectation-maximization (EM) algorithm, viewing the ``true'' cluster memberships as the missing or complete data \citep{McLachlan88, Dempster77}. 


It's useful to frame the problem as observing incomplete or censored data from a ``complete'' experiment where the components that generate each data point are known. Notationally, each "complete" data point is a pair $(\bx_i, \bz_i)$, where 
$\bz_i \in \reals^K$ define the cluster memberships, such that $z_{ik} = \ind{\bx_i \text{ was generated from component $k$}}$.  
If $K$ is fixed, parameters can be estimated using EM algorithm by incorporating the density of each complete data pair,
\ba
f(\bx_i, \bz_i | K, \btheta_K)
	& = \prod_{k=1}^K \left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}}
\ea
whose derivation is given in Appendix A.
The EM algorithm is appropriate here since the true component assignments $\bz_i$ can be considered as ``missing'' data on which we can compute conditional expectations given the observed data.



\subsection{Choosing the number of mixture components $K$}
The goal in many clustering problems is to choose the number of components $K$ either based on previous knowledge, or perhaps without any other information.
\citet{Dasgupta98} were the first to propose using the BIC to select the number of components, which in this context is synonymous with the number of clusters. 
That is, given a set of candidates $\set{K_\text{min}, \dotsc, K_\text{max}}$, compute the MLE estimates for each $K$, and then choose the $K$ which minimizes the BIC (Bayesian information criterion), defined as
\ba
\text{BIC}(K) 
	& = \log p(\bx | K, \estim\btheta_{K, \text{MLE}})
		- \frac{ \nu_K \log(n) }{2}.
\ea
Here,
$f(\bx | K, \btheta_K) = \prod_{i = 1}^n f(\bx_i | K, \btheta_K)$ is the density of the full dataset $\bx_1, \dotsc, \bx_n$, and
 $\nu_K$ is the number of parameters of the $K$-component model.
The BIC criterion can be viewed as an approximation to the integrated likelihood
\ba
\text{IL}(\bx | K)
	& = \int_{\Theta_K} f(\bx | K, \btheta_K) \der \pi(\btheta_K)
\ea
where $\pi(\cdot)$ is a prior distribution over the parameter space $\bTheta_K$.
Insofar as the groups are normally distributed, the BIC will typically estimate the number of components well.
However, the less normally distributed a group is, the BIC is more likely to approximate the group as a combination of multiple Gaussian groups.
This will lead to an overestimation of $K$ by the BIC when such groups are present in the data. 

\citet{Biernacki00} modify this approach by introducing the ICL criterion, namely,
\ba
\estim{\text{ICL}}(K)
	& \approx 
	\log f(\bx, \estim\bz | K, \estim\theta_{K, \text{MLE}}) + \frac{ \nu_K \log(n) }{2}
\ea
an approximation of the integrated complete likelihood of the data. 
Here, $f(\bx, \bz | K, \btheta) = \prod_{i=1}^n f(\bx_i, \bz_i | K, \btheta)$ is the complete density of the full dataset.
Since we don't know the values of $\bz_i$, we must plug in estimates $\estim\bz_i$ using, for example, maximum a posteriori (MAP) estimates.
Analogously to the BIC, this criterion approximates the integrated complete likelihood
\ba
\text{ICL}(\bx, \bz | K)
	& = \int_{\Theta_K} f(\bx, \bz | K, \btheta_K) \; \der \pi(\btheta_K).
\ea
It turns out that the ICL can also be seen as a penalization of the mean Shannon entropy of the fitted model with $K$ components, 
\ba
\text{Ent}(K)
	& = - \sum_{i=1}^n \sum_{k = 1}^K t_{ik}(\estim\btheta_K) \log t_{ik}(\estim\btheta_K)
\ea
The mathematical details of this decomposition be found in Appendix A.
Because of the entropy penalty, which favors well-separated clusters over connected or jumbled ones, the ICL criterion tends to choose a smaller $K$ than the BIC.
This often leads an underestimation of mixture components, but a potentially more reasonable estimated number of clusters.
Furthermore, as in the BIC case, the ICL is restricted to Gaussian models of possibly non-Gaussian groups.


\section{Methods}

\cite{Baudry10} focus on the model-based paradigm, assuming multivariate Gaussian mixture component densities. 
That is, given $K$ mixture components, we decompose the density of $\bx_i$ as
\ba
f(\bx_i | K, \btheta_K)
	& = \sum_{i=k}^K p_k \phi (\bx_i ; \mu_k, \Sigma_k ).
\ea
where $\phi(\cdot; \mu_k, \Sigma_k)$ is the $d$-variate Gaussian density with mean $\mu_k$ and covariance matrix $\Sigma_k$; and $\btheta_K = (\bp, \bmu, \bSigma) = (p_1, \dotsc, p_{K - 1}, \mu_1, \dotsc, \mu_k, \Sigma_1, \dotsc, \Sigma_K)$.
The choice of covariance is flexible, usually taken to be either spherical ($\Sigma_k = \sigma^2 I_d$), diagonal ($\Sigma_k = \text{diag}(\sigma_{k,1}, \dotsc, \sigma_{k,d})$), or general (unstructured).
Unsurprisingly, the choice of covariance induces a trade-off between accuracy and tractability, with the more general covariance structures allowing for potentially better model fits, but requiring significantly more parameters to estimate.



To utilize the respective strengths of the BIC and ICL criteria, \cite{Baudry10}  propose a hierarchical clustering sequence that minimizes the overall entropy during each merge.
In particular, at each iteration the algorithm determines the pair of clusters which, when merged, yields the largest decrease in entropy.
They suggest starting the algorithm with $K \gets \estim K_\text{BIC}$, the value of $K$ selected by the BIC criterion.

The algorithm can be formalized notationally as follows.
For a given $K$, let $t_{ik} = t_{ik}(\estim\theta_K)$ denote the conditional probability that data point $\bx_i$ belongs to cluster $k$.
Then for any pair of clusters $k$ and $k'$, it is immediate (see appendix B) that
\ba
t_{i, k\cup k'} = t_{ik} + t_{ik'}.
\ea
with the remaining $t_{ij}, j \not\in \set{k, k'}$, unchanged.
Thus, the corresponding entropy of an individual point $\bx_i$ with respect to the new, merged cluster $k \cup k'$ is
\ba
t_{i, k\cup k'} = \left(t_{ik} + t_{ik'}\right) \log\left( t_{ik} + t_{ik'} \right)
\ea
This is distinct from the entropy of $\bx_i$ with respect to the separate clusters $k$ and $k'$,
\ba
t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'}.
\ea
This induces a difference in overall entropy when merging $k$ and $k'$, yielding
\ba
\Delta\text{Ent}( k, k') :=
\sum_{i=1}^n t_{i,k\cup k'} \log t_{i, k \cup k'}
- \sum_{i=1}^n \left\{ t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'} \right\}.
\ea
Thus, at each iteration, the algorithm merges clusters $(k, k') = \argmax_{j, \ell} \Delta\text{Ent}(j, \ell)$, reducing the number of clusters to $K - 1$ and giving rise to new estimates of $t_{ik}, 1 \le k \le K - 1$. 

In the BIC and ICL approaches, clusters exactly correspond to Gaussian mixture components.
\cite{Baudry10} transcend this restriction by allowing clusters to be Gaussian mixtures to better approximate non-Gaussian cluster shapes.
In other words, cluster definitions are allowed to be sub-mixtures within the full Gaussian mixture.
This should in principle increase the separation between clusters, and hence decrease the overall entropy.
The authors demonstrate superior performance empirically through various simulations, showing that their method leads to fewer point misclassifications and notably lower entropy from both the BIC and ICL solutions.

The method of \cite{Baudry10} yields a sequence of clustering solutions for each value of $K \in \set{K_\text{min}, \dotsc, \estim K_\text{BIC}}$.
Given this sequence, the modeler may wish to choose the ``best'' clustering for the problem at hand.
The authors suggest several approaches for automatically selecting $K$, including:
\begin{enumerate}
\item
choosing a $K$ substantially either using the scientific background of the data, or simply visualizing the clusters with the data;
\item
choosing $K = K_\text{ICL}$, since the ICL criterion tends to choose the number of clusters effectively;
\item
regressing the normalized change in entropy on $K$, and identifying an ``elbow'' to indicates the entropy has leveled out.
\end{enumerate}
Here, the normalized change in entropy is defined to be
\ba
\Delta\text{Ent}_\text{norm}(K)
	:= \frac{ \text{Ent}(K + 1) - \text{Ent}(K) }{ \text{card}\left(k \cup k'\right) } 
\ea
where $k\cup k'$ is the newly merged cluster from previously separate clusters $k$ and $k'$.
That is, we scale the difference in entropy by the number of observations involved in the merging process from the $(K+1)$-cluster solution to the $K$-cluster solution.
Thus, we can automatically infer $K$ by performing a piecewise linear regression $\Delta \text{Ent}_\text{norm}(K) \sim K$ and set $K$ to be the estimated breaking point, or elbow.
A cluster merge involving a large number of observations will lead to a larger difference in entropy; normalizing the entropy difference as above attempts to alleviate this bias so that a merge's effectiveness relies only on the corresponding increase in separation.
One issue that may arise is that the piecewise linear regression may identify multiple elbows, implying a hierarchical nature of clustering.
One can automatically choose $K$ in this case, such as always choosing the smallest such breaking point.
Nonetheless, a general method to automatically select $K$ appears difficult, and may depend on the context and subjective qualities of the different clusterings.

\section{Results}
\cite{Baudry10} illustrate their method with several simulation studies and apply it to real datasets from a flow cytometry experiment.
We reproduce each of these experiments below, commenting on differences between implementations when they arise.
Each Gaussian mixture model was fit using the popular \texttt{mclust} package, which allows selection of $K$ based on both the BIC and ICL criteria.
A custom implementation of the EM algorithm was also created for completeness, but the performance is vastly inferior to \texttt{mclust} which uses highly involved and specialized algorithms for both the initialization and main steps of the EM algorithm \citep{Scrucca16, Fraley96}.

\subsection{Gaussian mixture with overlapping components}
First, we simulate data from a mixture of bivariate Gaussian densities that includes six components.
The raw, unlabelled data are shown below, representing 600 samples from the mixture.
It is seen from the plot that both the upper-left and lower-right groups correspond two overlapping component densities, forming ``cross'' clusters.
The other two component densities represent separate, individual groups.
The mixture proportions of the components in the lower cross were $1/10$ each, and the mixture proportion of every other component was $1/5$. 
The unclustered data are seen in Figure \ref{Example1}(a).

\begin{figure}
\begin{center}
\subfloat[Unclustered data]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/unclustered.pdf}
}
\subfloat[BIC solution]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_6_contour.pdf}
}
\subfloat[ICL solution]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/ICL_contour.pdf}
}
\\
\subfloat[Merged solution, $K = 6$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_6_contour.pdf}
}
\subfloat[Merged solution, $K = 5$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_5_contour.pdf}
}
\subfloat[Merged solution, $K = 4$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_4_contour.pdf}
}
\\
\subfloat[Merged solution, $K = 3$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_3_contour.pdf}
}
\subfloat[Merged solution, $K = 2$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_2_contour.pdf}
}
\subfloat[Merged solution, $K = 1$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_1_contour.pdf}
}
\end{center}
\caption{
The unclustered data as well as labels and contours corresponding to each of the BIC, ICL, and merged solutions. Note that the BIC and $K = 6$ merged solution coincide.
}
\label{Example1}
\end{figure}

Gaussian mixture models with general covariance structure were fit to these data for $K = 1, \dotsc, 12$.
The BIC selected $\estim K_{\text{BIC}} = 6$, the true number of components, and the ICL selected $\estim K_{\text{ICL}} = 4$.
These solutions are shown in Figure \ref{Example1} (b-c), respectively.
The merging procedure was applied, starting with $\estim K_\text{BIC} = 6$ components, and terminating after computing the $K = 1$ solution.
Each clustering in the sequence is shown in Figure \ref{Example1}.

Next, we illustrate the suggested entropy regression methodology.
Figure \ref{Entropy1} shows four plots.
The first displays the total entropy of the clustering versus the number of components $K$, as well as two line segments corresponding to the piecewise regression that minimizes the total sum of squared errors (SSE).
That is, we consider each possible change point $c \in \set{2, \dotsc, \estim K_\text{BIC} - 1}$, regress separately on $\set{1, \dotsc, c}$ and $\set{c, \dotsc, K}$, compute SSE$(c)$, and choose $\estim c = \argmin_c \text{SSE}(c)$.
The second plot displays the difference in entropy when merging from $K + 1$ to $K$ versus $K$.
The third plot displays the total entropy versus the cumulative count of merged observations.
The fourth plot displays the normalized difference in entropy when going from $K$ to $K - 1$ versus $K$.
Each of these plots suggests $K = 4$ as a reasonable change point, and hence a reasonable choice for the number of clusters.

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/diff_entropy.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/norm_diff.pdf}
}
\end{center}
\caption{Entropy plots for example 3.2}
\label{Entropy1}
\end{figure}

\subsection{Gaussian mixture with overlapping components and restricted models}
The next simulation study applies the previous techniques to data sampled from the same mixture model as in the previous section, but restricts the candidate models by assuming diagonal variances.
The goal is to assess the performance of the merging technique under model misspecification.

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/unclustered.pdf}
}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_11_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/ICL_contour.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_10_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_9_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_8_contour.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_7_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_6_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_5_contour.pdf}
}
\\
\subfloat[]{
		\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_4_contour.pdf}
}
\end{center}
\label{Example2}
\end{figure}

We also show entropy plots, and include piecewise regressions with one change point for the plots of total entropy.

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/diff_entropy.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/norm_diff.pdf}
}
\end{center}
\end{figure}

\subsection{Mixture of a bivariate uniform and a bivariate, spherical Gaussian}
Next, we investigate the behavior of the merging method when non-Gaussian groups are present in the data.
In particular, we simulate a two-component mixture where one component is uniform on $[-1, 1]^2$ and one component is a spherical, bivariate Gaussian.

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3/unclustered.pdf}
}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3/merged_2_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3/ICL_contour.pdf}
}
\end{center}
\end{figure}

\subsection{Comparison with Li's method using a Gaussian mixture}
The next two examples are direct comparisons with a similar method laid out by \cite{Li05}.
In this method, clusters are also modeled as Gaussian mixtures, although the true number of clusters $K$ is assumed to be known in advanced.
The algorithm is initialized using a two-layer $k$-means technique: one run of $k$-means for the $K$ clusters, and then a further $k$-means run within each cluster.
Then, within each cluster, the algorithm again performs $k$-means, and uses the BIC to select the number of components. 
The algorithm then merges clusters to minimize an inertia, or sum-of-squares, criterion; this contrasts with the entropy criterion of \cite{Baudry10}.
The total number of components is selected using the BIC.

 \begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d41/unclustered.pdf}
}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d41/merged_4_contour.pdf}
}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d41/merged_3_contour.pdf}
}
\end{center}
\end{figure}

\subsection{Comparison with Li's method using a trivariate uniform mixture with overlapping components}
The second comparison with the method of \cite{Li05} uses a trivariate uniform mixture composed of two distinct groups: a horizontally-oriented ``cross'' and a vertically-oriented ``pillar''.
See INSERT FIGURE HERE for a 3-dimensional visualization and 2-dimensional projection of the data.
We fit general Gaussian mixture models and apply our merging technique.

 \begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_1_scatter.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_1_top_2d.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_3_scatter.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_3_top_2d.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_2_scatter.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_2_top_2d.pdf}
}
\end{center}
\end{figure}


\subsection{Flow cytometry study}

Now we examine datasets from \cite{Brinkman07}.
To identify CD3$^+$CD4$^+$CD8$\beta^+$ populations, we follow the strategy of \cite{Lo08}.

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d51_raw/merged_13_CD3_CD8beta.pdf}
}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d51_raw/ICL_CD3_CD8beta.pdf}
}
\subfloat[]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d51_raw/merged_8_CD3_CD8beta.pdf}
}
\end{center}
\end{figure}

\section{Discussion}

\bibliography{stat572}

\section*{Appendix A: Theoretical foundations for ICL}
The likelihood for one observation is
\ba
f(\bx_i | K, \theta_K)
	& = \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k)
\ea
Thus, assuming independence, the observed likelihood is
\ba
f(\bx | K, \theta_K)
	& = \prod_{i=1}^n \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k)
\ea
That is, the observed log-likelihood is
\ba
L(\theta_K | \bx, K)
	& = \log\left( \prod_{i=1}^n \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right) \\
	& = \sum_{i=1}^n \log \left( \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
\ea
Now, noting that for a unit vector $\bz_i \in \reals^K$ with $k$th element 1, 
\ba
\Pr{\bz_i} = \Pr{z_{ik} = 1} = p_k
\ea
the complete likelihood for one observation is
\ba
f(\bx_i, \bz_i)
	& = f(\bx_i | \bz_i) \Pr{\bz_i} \\
	& = \sum_{k=1}^K \phi(\bx_i | \mu_k, \Sigma_k) \ind{\bz_{ik} = 1}
		p_k \\
	& = \prod_{k = 1}^K \left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}}
\ea
so that the complete likelihood is
\ba
f(\bx, \bz| K, \theta_K)
	& = \prod_{i=1}^n \prod_{k = 1}^K 
		\left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}} \\
\ea
yielding the complete log-likelihood
\ba
\text{CL}(\theta_K | \bx, \bz, K)
	& = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \log \left( p_k \phi(\bx_i | \mu_k, \Sigma_k) \right).
\ea
Let $t_{ik}$ be the conditional probability that $\bx_i$ comes from component $k$.
Then from Bayes's theorem,
\ba
t_{ik}
	& = t_{ik}(K, \btheta_K) \\
	& = \Pr{\text{component } k \mid \bx_i, K, \btheta_K} \\
	& = \frac{\Pr{ \text{component }k, \bx_i \mid K, \btheta_K}}
		{f(\bx_i|K,\btheta_K)} \\
	& = \frac{\Pr{ \bx_i | \text{component } k, K, \btheta_K }
		\Pr{\text{component } k | K, \btheta_K}}
		{ \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) } \\
 	& = \frac{ \phi(\bx_i | \mu_k, \Sigma_k) \cdot p_k }
 	{ \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) }
\ea 
If we define
\ba
\text{EC}(K | \bz) := - \sum_{k=1}^K \sum_{i=1}^n z_{ik} \log t_{ik} 
\ea
We see that
\ba
L(\theta_K) - \text{EC}(K)
	& = \sum_{i=1}^n \log\left( \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		- \left( - \sum_{k=1}^K \sum_{i=1}^n z_{ik} \log(t_{ik}) \right) \\
	& = \sum_{i=1}^n \left\{
		\log\left(\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		+ \sum_{k=1}^K z_{ik} \log\left( \frac{p_k \phi(\bx_i | \mu_k, \Sigma_k)}{ \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) } \right)
		\right\} \\
	& = \sum_{i=1}^n \Bigg\{
		\log\left(\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		\\
	& \htab + \sum_{k=1}^K z_{ik} \left[ \log\left( p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		- \log \left( \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) \right) \right]
		\Bigg\} \\
	& = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \log(p_k \phi(\bx_i | \mu_k, \Sigma_k))
		+ \sum_{i=1}^n \log\left(\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
			\underbrace{\left[ 1 - \sum_{i=1}^K z_{ik} \right]}_{=0} \\
	& = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \log(p_k \phi(\bx_i | \mu_k, \Sigma_k)) \\
	& \equiv \text{CL}(K)
\ea
Seeing EC$(K | \bZ)$ as a random variable, we have for a fixed $K$,
\ba
\E{ \text{EC}(K | \bZ) } & = - \sum_{k=1}^K \sum_{i=1}^n \E{Z_{ik}} \log t_{ik} \\
	& = - \sum_{k=1}^K \sum_{i=1}^n \Pr{Z_{ik} = 1} \log t_{ik} \\
	& = - \sum_{k=1}^K \sum_{i=1}^n \Pr{X_i \text{ comes from $k$th component} } \log t_{ik} \\
	& \equiv -  \sum_{k=1}^K \sum_{i=1}^n t_{ik} \log t_{ik} \\
	& = \text{Ent}(K),
\ea
the entropy of the matrix $\bT = \set{t_{ik}}$.

Now, the integrated likelihood (aka evidence or model evidence) is
\ba
\text{IL}(\bx | K, \theta_K)
	& = \int_{\Theta_{K}} f(\bx | K, \theta_K) \pi(\theta_K | K) \; \der \theta_K \\
	& = \int_{\Theta_{K}} \prod_{i=1}^n \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \pi(\theta_K | K) \; \der \theta_K
\ea
An approximation is given via
\ba
	\log \text{IL}(\bx | K, \theta_K) 
		& \approx \log f(\bx | K, \estim\theta_{K, \text{MLE}}) - \frac{\nu_K \log(n)}{2} \\
		& \equiv \text{BIC}(K)
\ea
where $\estim\theta_{MLE} = \argmax_\theta f(\bx | K, \theta)$. 
Analogously, we define the integrated complete likelihood as
\ba
\text{ICL}(\bx, \bz | K)
	& = \int_{\Theta_K} f(\bx, \bz | K, \theta_K) \pi(\theta_K | K) \; \der \theta_K \\
	& = \int_{\Theta_K} \prod_{i=1}^n \prod_{k = 1}^K 
		\left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}} \pi(\theta_K | K) \; \der \theta_K \\
\ea
which takes into account evidence for an effective clustering structure. 
We can introduce a similar approximation to BIC for missing data:
\ba
\log\text{ICL} \approx \log f(\bx, \estim\bz | K, \estim\theta_{K, \text{MLE}}) + \frac{ \nu_K \log(n) }{2}
\ea
where
\ba
\estim z_{ik} = \ind{ \argmax_{1 \le \ell \le K} t_{i \ell}(\estim\theta_{MLE}) = k }. 
\ea

\section*{Appendix B: Foundations of Baudry et al. methodology}
Suppose we merge clusters $k$ and $k'$ to form $k \cup k'$. Then
\ba
t_{i, k \cup k'}^K
	& = \Pr{\bx_i \text{ comes from $(k \cup k')$th component}} \\
	& = \Pr{\bx_i \text{ comes from $k$th or $k'$th component} } \\
	& = \Pr{\bx_i \text{ comes from $k$th component }} + \Pr{\bx_i \text{ comes from $k'$th component} } \\
	& = t_{ik}^K + t_{ik'}^K
\ea
Then the resultant entropy (after relabeling the indices) is
\ba
\text{Ent}(K - 1) & = - \sum_{k=1}^{K - 1} \sum_{i=1}^n t_{ik} \log t_{ik} \\
	& = - \sum_{i=1}^n \left( \sum_{j \ne k \cup k'} t_{ij} \log(t_{ij}) + t_{i,k \cup k'} \log( t_{i,k\cup k'} ) \right) \\
	& = - \sum_{i=1}^n \left( \sum_{j \ne k \cup k'} t_{ij} \log(t_{ij}) + (t_{i,k}^K + t_{i,k'}^K) \log( t_{ik}^K + t_{ik'}^K) \right).
\ea
Then we have the corresponding difference in entropy:
\ba
\Delta(K) 
	& \equiv \text{Ent}(K) - \text{Ent}(K - 1) \\
	& = -  \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log t_{ij} + \sum_{i=1}^n \sum_{j=1}^{K - 1} t_{ij} \log t_{ij} \\
	& = - \sum_{i=1}^n \left\{
			\sum_{j \ne k, k'} t_{ij} \log t_{ij}
			+ t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'}
			+ \sum_{j \ne k \cup k'} t_{ij} \log t_{ij}
			+ t_{i, k\cup k'} \log t_{i, k \cup k'}
		\right\} \\
	& = - \sum_{i=1}^n \left\{ t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'} \right\}
		+ \sum_{i=1}^n t_{i,k\cup k'} \log t_{i, k \cup k'}
\ea
Thus, we choose clusters $k$ and $k'$ such that combining them maximizes $\Delta(K)$, i.e., yields the highest decrease in entropy moving from $K$ to $K - 1$ clusters, which is good since low entropy means a well-partitioned model 

\section*{Appendix C: Expressions for $\nu_K$}
Recall the BIC criterion, defined as
\ba
\text{BIC}(K) 
	& = \log p(\bx | K, \estim{\theta_K})
		- \frac{\nu_K}{2} \log(n).
\ea
If we assume general covariance matrices $\Sigma_k$, then the number of model parameters is
\ba
\nu_K & = \dim \mathcal M_K \\
	& = \dim \theta_K \\
	& = \dim \left( p_1, \dotsc, p_{K - 1}, \bolda_1, \dotsc, \bolda_K \right) \\
	& = \dim \left( p_1, \dotsc, p_K \right)
		+ \dim \left( \bolda_1, \dotsc, \bolda_K \right) \\
	& = K - 1 + K \dim(\bolda_1) \\
	& = K - 1 + K \left(\dim\left(\mu_1, \Sigma_1 \right) \right) \\
	& = K - 1 + K \left[ m + m(m + 1)/2 \right] \\
	& = K\left[1 + m + \frac{m(m + 1)}{2} \right] - 1. 
\ea 

\section*{Appendix D: Miscellaneous calculations}
Conditional model probabilities given the data:
\ba
\Pr{M_\ell | \bx} & = \frac{f(\bx | M_\ell) \Pr{M_\ell}}{ f(\bx) } \\
	& = \frac{f(\bx | M_\ell) \Pr{M_\ell}}{ \sum_{r = 1}^m f(\bx | M_r) \Pr{M_r} } \\
\ea
If $M_1 = \dotsc = M_m$, then
\ba
\Pr{M_\ell | \bx} & = \frac{f(\bx | M_\ell)}{ \sum_{r=1}^m f(\bx | M_r) }.
\ea

\subsection*{Mean of a Gaussian submixture}
A cluster is considered positive in a biomarker $B$ if its estimated mean $\mu_B$ is $\ge 280$ units.
In order to compute its mean, we need to derive the expression for a mean of a submixture of Gaussians, which can be done via the following.
Let $f_s$ be a density corresponding to a submixture of $m \le K$ Gaussian component densities.
Then WLOG we can write
\ba
f_s(\bx) = \sum_{j=1}^m p_j \phi_j(\bx | \mu_j, \Sigma_j).
\ea
Let $X \sim f_s$ be a sample from this density, i.e., a sample from the overall mixture conditioned on the component belonging to this particular submixture.
Then the conditional probability that the generating component is $j$ is a multinomial, with probability $\frac{p_j}{\sum_{k=1}^m p_k}$.
Then we have by the law of total expectation,
\ba
\E{X}
	& = \sum_{j=1}^m \E{X \mid \text{component} = j }
			\Pr{ \text{component} = j } \\
	& = \sum_{j=1}^m \mu_j \cdot \frac{p_j}{ \sum_k^m p_k } \\
	& = \frac{ \sum_{j=1}^m p_j \mu_j }{ \sum_{j=1}^m p_j }
\ea
which verifies the claim.



\end{document}









