\documentclass{uwstat572}

%%\setlength{\oddsidemargin}{0.25in}
%%\setlength{\textwidth}{6in}
%%\setlength{\topmargin}{0.5in}
%%\setlength{\textheight}{9in}

\renewcommand{\baselinestretch}{1.5} 

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{array}
\usepackage{bbm}
\usepackage{cancel}
\usepackage{courier}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{ mathrsfs }
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{setspace}

\pagenumbering{gobble}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}

\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\mean[1]{\overline{#1}}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\card[1]{\text{card}\left(#1\right)}
\newcommand*\D{\mathscr{D}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\;\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\;\mathsf{Var}\left(#1\right)}
\newcommand*\SD[1]{\;\mathsf{SD}\left(#1\right)}
\newcommand*\SE[1]{\;\mathsf{SE}\left(#1\right)}
\newcommand*\Cov[1]{\;\mathsf{Cov}\left(#1\right)}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[1]{\mathbbm{1}\left(#1\right)}
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\deriv[2]{\frac{\der #1}{\der #2}}
\newcommand*\pderiv[2]{\frac{\pd #1}{\pd #2}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}
\newcommand*\bolda{\mathbf{a}}
\newcommand*\bA{\mathbf{A}}
\newcommand*\bb{\mathbf{b}}
\newcommand*\bB{\mathbf{B}}
\newcommand*\bc{\mathbf{c}}
\newcommand*\bD{\mathbf{D}}
\newcommand*\be{\mathbf{e}}
\newcommand*\bG{\mathbf{G}}
\newcommand*\bh{\mathbf{h}}
\newcommand*\bI{\mathbf{I}}
\newcommand*\bell{\boldsymbol{\ell}}
\newcommand*\bL{\mathbf{L}}
\newcommand*\bN{\mathbf{N}}
\newcommand*\bo{\mathbf{o}}
\newcommand*\bp{\mathbf{p}}
\newcommand*\br{\mathbf{r}}
\newcommand*\bR{\mathbf{R}}
\newcommand*\bs{\mathbf{s}}
\newcommand*\bS{\mathbf{S}}
\newcommand*\bt{\mathbf{t}}
\newcommand*\bT{\mathbf{T}}
\newcommand*\bu{\mathbf{u}}
\newcommand*\bv{\mathbf{v}}
\newcommand*\bV{\mathbf{V}}
\newcommand*\bx{\mathbf{x}}
\newcommand*\bw{\mathbf{w}}
\newcommand*\bW{\mathbf{W}}
\newcommand*\bX{\mathbf{X}}
\newcommand*\by{\mathbf{y}}
\newcommand*\bY{\mathbf{Y}}
\newcommand*\bZ{\mathbf{Z}}
\newcommand*\bz{\mathbf{z}}
\newcommand*\bzero{\mathbf{0}}
\newcommand*\bone{\mathbf{1}}
\newcommand*\balpha{\boldsymbol{\alpha}}
\newcommand*\bbeta{\boldsymbol{\beta}}
\newcommand*\bgamma{\boldsymbol{\gamma}}
\newcommand*\bdelta{\boldsymbol{\delta}}
\newcommand*\bDelta{\boldsymbol{\Delta}}
\newcommand*\beps{\boldsymbol{\varepsilon}}
\newcommand*\btheta{\boldsymbol{\theta}}
\newcommand*\bTheta{\boldsymbol{\Theta}}
\newcommand*\bmu{\boldsymbol{\mu}}
\newcommand*\bpsi{\boldsymbol{\psi}}
\newcommand*\bsigma{\boldsymbol{\sigma}}
\newcommand*\bSigma{\boldsymbol{\Sigma}}
\newcommand*\btau{\boldsymbol{\tau}}
\newcommand*\bOmega{\boldsymbol{\Omega}}
\newcommand\Psub[2]{\mathsf{Pr}_{#1}\left(#2\right)}
\newcommand\Vsub[2]{\mathsf{Var}_{#1}\left(#2\right)}
\newcommand\e{\operatorname{e}}
\newcommand\prox{\operatorname{prox}}
\newcommand\T{\mathsf{T}}

\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right) }


\bibliographystyle{plainnat}

\begin{document}
%%\maketitle

\begin{center}
  {\LARGE A Report On ``Combining Mixture Components for Clustering'' by Baudry et al.}\\\ \\
  {Branden Olson\\ 
    Department of Statistics, University of Washington Seattle, WA, 98195, USA
  }
\end{center}



\begin{abstract}
Model-based clustering typically assumes an underlying Gaussian mixture model in which the number of mixture components is unknown. 
The familiar Bayesian information criterion excels at inferring the number of components when the Gaussian approximation holds.
However, the presence of non-Gaussian groups violates the assumption that each component represents its own cluster, leading to an overestimation of the number of groups.
A modification known as the integrated complete likelihood criterion penalizes model entropy to more accurately infer the number of groups, but can also misconstrue non-Gaussian groups as Gaussian and underestimate the number of components.
\citet{Baudry10} resolve this dilemma by starting with the BIC solution and iteratively merging components to minimize entropy.
This technique produces clusters that are Gaussian mixtures themselves, allowing the number of groups and mixture components to differ.
The result is a hierarchical sequence of clusterings from which the modeler can choose a preferred representative based on various criteria.
In this report we reproduce their results, including several simulation studies as well as flow cytometry data, and demonstrate competitive performance in each case.
\end{abstract}

\section{Introduction}

\subsection{Model-based clustering}
Many situations arise in which collected data points must be clustered into similar subgroups. 
For example, we might wish to cluster DNA sequences from a population into phylogenetic families, or categorize X-ray data readings by the galaxies from which the waves originated.
In general, we consider clusters to be contiguous, densely-populated regions of a feature space, separated by contiguous, relatively empty regions.
The problem of clustering can be quite challenging as the usual difficulties of modeling the underlying distribution are compounded by the absence of information about the nature of the groups.
Indeed, the field of clustering contains a vast set of approaches based on various assumptions for the data-generating mechanism and cluster specifications.

One approach known as model-based clustering assumes that the data $\bx = (\bx_1, \dotsc, \bx_n)$, $\bx_i \in \reals^d$, were generated from a mixture model of probability distributions and can be assigned cluster labels based on their most likely mixture components.
Specifically, we decompose the probability density function $f$ of each point $\bx_i$ as a sum of scaled component densities:
\begin{equation}\label{density}
f(\bx_i | K, \btheta_K)
	 = \sum_{k=1}^K p_k \; \phi(\bx_i ; \btheta_k).
\end{equation}
Here, $\set{p_k}_{k = 1}^K$ are positive mixture proportions which sum to one, $\phi(\cdot, \btheta)$ is a density function parametrized by $\btheta$, and
 $\btheta_K$ contains all mixture proportions and component parameters within the $K$-component model.
Modelers commonly choose Gaussian component densities, often with success \citep{Murtagh84, Banfield93, Dasgupta98, Campbell97, Celeux95, Mukerjee98}.
However, other distributions may better suit the nature of the data depending on the context.
For example, the $t$-density is appropriate in the presence of outliers which heavily influence cluster definitions \citep{Peel00}.
For our purposes we take $\phi$ to be the $d$-variate Gaussian density so that the $k$th component has mean vector $\mu_k \in \reals^d$ and covariance matrix $\Sigma_k \in \reals^{d\times d}$.
The assumed covariance is flexible, often either spherical ($\Sigma_k = \sigma^2 I_d$), diagonal ($\Sigma_k = \text{diag}(\sigma_{k,1}, \dotsc, \sigma_{k,d})$), or general (unstructured symmetric).
We define $\btheta_K = \left(\bp, \bmu, \bSigma\right) = \left(p_1, \dotsc, p_K, \mu_1, \dotsc, \mu_K, \Sigma_1, \dotsc, \Sigma_K\right)$, and assume independence of samples so that $f(\bx | K, \theta_K) = \prod_{i=1}^n f(\bx_i | K, \theta_K)$.

Consider the observations $\bx$ as an incomplete or censored sample from a ``complete'' experiment in which the component assignments for each data point are known. Notationally, each complete data point is a pair $(\bx_i, \bz_i)$, where 
$\bz_i = (z_{i1}, \dotsc, z_{iK}) \in \reals^K$ with $z_{ik} = \ind{\bx_i \text{ was generated from component $k$}}$.
In this way, $\bz_i$ defines the cluster membership of $\bx_i$.
For a given $K$, parameters $\btheta_K$ can be estimated using the expectation-maximization (EM) algorithm with the ``completed'' density,
\begin{equation}\label{completedensity}
\begin{split}
f(\bx, \bz | K, \btheta_K)
	& = \prod_{i=1}^n f(\bx_i, \bz_i | K, \btheta_K) \\
	& = \prod_{i=1}^n \prod_{k=1}^K \left[ p_k \phi\left(\bx_i | \mu_k, \Sigma_k\right) \right]^{z_{ik}}.
\end{split}
\end{equation}
The EM algorithm is appropriate here since the true component assignments $\bz = (\bz_1, \dotsc, \bz_n)$ act as latent or ``missing'' variables on which we can compute conditional expectations given the observed data $\bx$ \citep{McLachlan88, Dempster77}.
More details concerning the relationships of \eqref{density} and \eqref{completedensity} are given in Appendix A.



\subsection{Model selection}
A principal goal in model-based clustering is to choose the number of components $K$ either based on previous knowledge or perhaps without any other information.
\citet{Dasgupta98} advocated using the Bayesian information criterion (BIC), a widely used tool in statistical model selection, to select $K$. 
That is, given a set of candidates $\set{K_\text{min}, \dotsc, K_\text{max}}$, we compute the MLE estimates for each $K$, and then choose $\estim K = \argmax_K \text{BIC}(K)$, where
\begin{equation}\label{BIC}
\text{BIC}(K) 
	= \log f \left(\bx \;\big|\; K, \estim\btheta_{K, \text{MLE}}\right)
		- \frac{ \nu_K \log(n) }{2}.
\end{equation}
The first term is simply the observed log-likelihood of the $K$-component model, and the second term contains $\nu_K$, the number of free parameters in this model.
Thus, the BIC comprises a tradeoff between model fit and model complexity.
This criterion approximates the integrated likelihood
\begin{equation}\label{IL}
\text{il}(\bx | K)
	 = \int_{\Theta_K} f(\bx | K, \btheta_K) \; \der \pi(\btheta_K)
\end{equation}
where $\pi(\cdot)$ is a prior distribution over the parameter space $\bTheta_K$.
Insofar as the Gaussian approximation holds, the BIC will typically estimate the number of components well.
However, non-Gaussian groups will be approximated as combinations of multiple Gaussian components, leading to an overestimation of the number of groups by the BIC. 

\citet{Biernacki00} modify this approach by introducing the integrated complete likelihood (ICL) criterion, namely,
\begin{equation}
\text{ICL}(K)
	= 
	\log f \left(\bx, \estim\bz \;\big|\; K, \estim\theta_{K, \text{MLE}}\right)
		 + \frac{ \nu_K \log(n) }{2}.
\end{equation}
Here, $f(\bx, \bz | K, \btheta) = \prod_{i=1}^n f(\bx_i, \bz_i | K, \btheta)$ is the complete density of the full dataset.
Since we don't know the values of $\bz_i$, we plug in maximum a posteriori (MAP) estimates $\estim\bz_i$ given by \eqref{map}.
Analogously to the BIC, this criterion approximates the theoretical integrated \emph{complete} likelihood
\begin{equation}
\text{icl}(\bx, \bz | K)
	 = \int_{\Theta_K} f(\bx, \bz | K, \btheta_K) \; \der \pi(\btheta_K).
\end{equation}
for a prior $\pi(\cdot)$ over $\bTheta_K$.
It turns out that the ICL can also be seen as a penalization of the mean Shannon entropy of the fitted $K$-component model, 
\begin{equation}
\text{Ent}(K)
	 = - \sum_{i=1}^n \sum_{k = 1}^K t_{ik}\left(\estim\btheta_K \right) 
	 	\log t_{ik}\left(\estim\btheta_K \right).
\end{equation}
We explicate the relationship of BIC, ICL, and entropy in Appendix A.
Because of the entropy penalty, which favors well-separated clusters over connected or jumbled ones, the ICL criterion tends to choose a smaller $K$ than the BIC.
This often leads an underestimation of mixture components, but a potentially more accurate number of groups.
Furthermore, as in the BIC case, the $\text{ICL}$ is restricted to Gaussian models of possibly non-Gaussian groups, and can cluster poorly separated components into one group.


\section{Methodology}
Despite the advantages of model-based clustering, sometimes the distribution of a group is simply not approximated well by a Gaussian component density.
To address this, \cite{Baudry10} propose a hierarchical clustering technique which harnesses some of these advantages while allowing components to be Gaussian mixtures themselves. 
The procedure starts with the model-based clustering solution chosen by BIC and iteratively merges components based on the entropy criterion underlying the ICL.
This results in a sequence of clusterings for each $K$ in the set $\set{K_\text{min}, \dotsc, \estim K_\text{BIC}}$, from which the modeler can choose a representative based on several principles which we discuss below.
In this way, \cite{Baudry10} exploit the respective strengths of the two criteria and overcome limitations of the Gaussian assumption.

The algorithm is formalized as follows.
For a given $K$, let $t_{ik} = t_{ik}\left(\estim\theta_K\right)$ denote the conditional probability that data point $\bx_i$ arose from component $k$.
This probability is explicitly given by \eqref{tik} when each component is Gaussian.
Then for any pair of clusters $k$ and $k'$, it is immediate that
\begin{equation}\label{tik_union}
t_{i, k\cup k'} = t_{ik} + t_{ik'}.
\end{equation}
with the remaining $t_{ij}, j \not\in \set{k, k'}$, unchanged.
Thus, the entropy contribution of an individual point $\bx_i$ with respect to the newly merged cluster $k \cup k'$ is
\ba
\left(t_{ik} + t_{ik'}\right) \log\left( t_{ik} + t_{ik'} \right).
\ea
This is distinct from the entropy contribution of $\bx_i$ with respect to the separate clusters $k$ and $k'$,
\ba
t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'}.
\ea
This induces a difference in overall entropy when merging $k$ and $k'$, yielding
\begin{equation}\label{delta_ent}
\Delta\text{Ent}( k, k'|K) :=
\sum_{i=1}^n t_{i,k\cup k'} \log t_{i, k \cup k'}
- \sum_{i=1}^n \left\{ t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'} \right\}.
\end{equation}
Thus, at each iteration, the algorithm merges clusters $(k, k') = \argmax_{1 \le j, \ell \le K} \Delta\text{Ent}(j, \ell|K)$, reducing the number of clusters to $K - 1$ and giving rise to new estimates of $t_{ik}, 1 \le k \le K - 1$.
The relationship \eqref{tik_union} holds when the components $k$ and $k'$ are Gaussian mixtures, as the conditional probability that $\bx_i$ arose from a mixture component $k$ is simply the sum of the conditional probabilities for the individual Gaussian components of $k$.
More details of this methodology are described in Appendix B.

Given the sequence of clusterings, one may wish to choose the ``best'' clustering for the problem at hand.
The authors suggest several approaches for selecting $K$, including:
\begin{enumerate}
\item
choosing $K$ substantially either using scientific background or by visualizing the clusters;
\item
setting $K = K_\text{ICL}$, since the ICL criterion tends to choose the number of clusters effectively;
\item
regressing entropy-based quantities on $K$, and identifying a change point or ``elbow'' to indicate when the entropy has leveled out.
\end{enumerate}
Concerning the third suggestion, \cite{Baudry10} describe four types of entropy relationships for plotting and regression.
The first two plots concern the relationships of total entropy and differences in entropy as $K$ decreases through the merging process.
Because a cluster merge involving a large number of observations will lead to a larger difference in entropy, the third plot attempts to alleviate this bias so that a merge's effectiveness relies only on the corresponding increase in separation.
In particular, it displays the total entropy versus the cumulative number of observations involved in the merging process.
The fourth plot examines the normalized change in entropy,
\begin{equation}
\Delta\text{Ent}_\text{norm}(K)
	:= \frac{ \text{Ent}(K + 1) - \text{Ent}(K) }{ \text{card}\left(k \cup k'\right) },
\end{equation}
where $k\cup k'$ is the newly merged cluster from previously separate clusters $k$ and $k'$ in the $K + 1$ solution.
Thus, we can automatically infer $K$ by estimating the change points, or elbows, in any of these plots.

For the two plots of total entropy, we follow \cite{Baudry10} and employ a piecewise linear regression by minimizing the total sum of squared error (SSE).
Specifically, we iterate over each possible change point $c \in \set{2, \dotsc, \estim K_\text{BIC} - 1}$, perform least squares regressions separately on $\set{1, \dotsc, c}$ and $\set{c, \dotsc, K}$, compute SSE$(c)$, and choose $\estim c = \argmin_c \text{SSE}(c)$.
While this piecewise fit is well-defined, it may not be optimal if the sample size of $K$-values is small (for example, if $c = 2$, we trivially interpolate a line through the values for $K = 1, 2$), and standard errors may be unavailable.
Moreover, a piecewise regression may identify multiple elbows, implying a hierarchical nature of clustering.
One can still automatically choose $K$ in this case, such as always choosing the smallest such breaking point, or taking the rounded midpoint.
Nonetheless, a general method to automatically select $K$ in every situation 	appears difficult, and the modeler must exercise judgment by incorporating context and subjective qualities of the different clusterings.

\section{Results}
\cite{Baudry10} illustrate their method with several simulation studies and apply it to real datasets from a flow cytometry experiment.
We reproduce each of these experiments below, commenting on differences in our results when they arise.
Each Gaussian mixture model was fit using the popular \texttt{mclust} package, which allows selection of $K$ based on both the BIC and ICL criteria.
Although a custom implementation of the EM algorithm has been created for completeness, we use \texttt{mclust} for our analyses due to its highly involved and specialized algorithms for both the initialization and main steps of the EM algorithm, and includes efficient versions of the procedure tailored to the many possible covariance assumptions \citep{Scrucca16, Fraley96}.

\subsection{Gaussian mixture with overlapping components}\label{one}
Our first example simulates from a mixture of bivariate Gaussian densities with six components.
The data are shown in Figure \ref{Example1}(a), representing 600 samples from the mixture.
As seen in the plot, the upper-left and lower-right groups correspond two overlapping component densities, forming ``cross'' clusters.
The other two component densities represent separate, individual groups.
The mixture proportions of the components in the lower cross were $1/10$ each, and the mixture proportion of every other component was $1/5$. 

\begin{figure}
\begin{center}
\subfloat[Unclustered data]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/unclustered.pdf}
}
\subfloat[BIC solution]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_6_contour.pdf}
}
\subfloat[ICL solution]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/ICL4_contour.pdf}
}
\\
\subfloat[Merged solution, $K = 6$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_6_contour.pdf}
}
\subfloat[Merged solution, $K = 5$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_5_contour.pdf}
}
\subfloat[Merged solution, $K = 4$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d1/merged_4_contour.pdf}
}
\end{center}
\caption{
The unclustered data as well as labels and contours corresponding to the BIC, ICL, and the first three merged solutions for example \ref{one}. Note that the BIC and $K = 6$ merged solution coincide by design.
}
\label{Example1}
\end{figure}

Gaussian mixture models with general covariance structure were fit to these data for $K = 1, \dotsc, 12$.
The BIC selected $\estim K_{\text{BIC}} = 6$, the true number of components, and the ICL selected $\estim K_{\text{ICL}} = 4$; the corresponding clustering solutions are shown in Figure \ref{Example1} (b-c), respectively.
Next, the merging procedure was applied, starting with the BIC solution and terminating with the $K = 1$ solution.
The clusterings for $K = $ 6, 5, and 4 are shown in Figure \ref{Example1} (d-f).
A table of inferred $K$ values and computed entropy values for each solution is shown in table \ref{Table1}.
This example illustrates the dilemma of model-based clustering, because while there are six true components, there are arguably four ``groups'' in terms of contiguity and well-separatedness.
An advantage of the merging technique is that it provides a satisfactory solution for both the $K = 6$ and $K = 4$ cases, with the $K = 4$ solution having a more honest density representation than the ICL solution.
\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/diff_entropy.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d1/norm_diff.pdf}
}
\end{center}
\caption{Entropy plots for example \ref{one}.}
\label{Entropy1}
\end{figure}


Next, we illustrate the entropy regression methodology via the four plots in figure \ref{Entropy1}.
The first displays the total entropy of the clustering versus the number of components $K$, as well as two line segments corresponding to the piecewise regression described in the Methodology section.
The second plot displays the difference in entropy when merging from $K + 1$ to $K$ versus $K$.
The third plot displays the total entropy versus the cumulative count of merged observations.
The fourth plot displays $\Delta \text{Ent}_\text{norm}(K)$ versus $K$.
Each of these plots suggests $K = 4$ as a reasonable change point, and hence a reasonable choice for the number of clusters.

\subsection{Gaussian mixture with overlapping components and restricted models}\label{two}
The next simulation study applies the same techniques to data sampled from the mixture model of the previous section, but restricts the candidate models by assuming diagonal, or axis-aligned, covariance matrices.
The goal is to assess the performance of the merging technique under model misspecification, since we know that two of the true component densities possess more general covariance matrices than the assumed model allows.
The unclustered data are shown in figure \ref{Example2} (a).

\begin{figure}
\begin{center}
\subfloat[Unclustered data]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/unclustered.pdf}
}
\subfloat[BIC solution, $K = 11$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_11_contour.pdf}
}
\subfloat[ICL solution, $K = 7$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/ICL7_contour.pdf}
}
\\ \vspace{-1em}
\subfloat[Merging solution, $K = 10$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_10_contour.pdf}
}
\subfloat[Merging solution, $K = 9$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_9_contour.pdf}
}
\subfloat[Merging solution, $K = 8$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_8_contour.pdf}
}
\\ \vspace{-1em}
\subfloat[Merging solution, $K = 7$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_7_contour.pdf}
}
\subfloat[Merging solution, $K = 6$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_6_contour.pdf}
}
\subfloat[Merging solution, $K = 5$]{
	\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_5_contour.pdf}
}
\\ \vspace{-1em}
\subfloat[Merging solution, $K = 4$]{
		\includegraphics[width=0.3\linewidth]{../Olson/Figures/d2/merged_4_contour.pdf}
}
\end{center}
\vspace{-1em}
\caption{
The unclustered data as well as labels and contours corresponding to the BIC, ICL, and merged solutions (until $K = 4$) for example \ref{two}.}
\label{Example2}
\end{figure}

The BIC chooses $K = 11$, nearly twice the number of actual components used to generate the data.
This is mostly due to the need to approximate the two diagonal Gaussian groups as a combination of three axis-aligned Gaussian densities.
The ICL also overestimates $K$ to be 7, and also has trouble delineating the true densities.
Applying the merging technique, we see that it first merges the cross clusters until they are each a distinct cluster.
Then it merges components in the diagonal groups until each is its own cluster.
This results in an accurate clustering for $K = 4$, whose contours approximate the true Gaussian contours well.
In particular, we see that the merging solution transcends the covariance restriction to infer accurate densities. 

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/diff_entropy.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d2/norm_diff.pdf}
}
\end{center}
\caption{Entropy plots for example \ref{two}.}
\label{Entropy2}
\end{figure}

Figure \ref{Entropy2} shows the corresponding entropy plots, which exhibit discrepancies over the selection of a change point.
In particular, the piecewise regression in the top left plot suggests $K = 8$, the plots of the difference and normalized difference in entropy both suggest $K = 4$, and the total entropy plot versus the cumulative count of merged observations suggests $K = 5$.
This is likely due to the hierarchical nature of the clustering sequence.
The top left plot of total entropy versus $K$ elucidates this, as it indicates two change points: one at $K = 8$ and one at $K = 4$.
These points correspond to when the merging procedure has formed the ``cross'' clusters and the diagonal clusters, respectively. 
Hence, a three-segment piecewise regression would seem more relevant here.

On the other hand, opting instead for $\estim K = \estim K_{\text{ICL}} = 7$ seems erroneous with respect to both the true number of components and the apparent number of contiguous, well-separated groups.
This exemplifies the difficulty of automatically choosing $K$.
In this case, a visual inspection of the data and resultant clusterings seems to be the best way to estimate $\estim K$.

\subsection{Mixture of a bivariate uniform and a bivariate, spherical Gaussian}\label{three}
Next, we investigate the behavior of the merging method when non-Gaussian groups are present in the data.
In particular, we simulate a two-component mixture where one component is uniform on $[-1, 1]^2$ and one component is a spherical, bivariate Gaussian with mean $\mu = (3, 0)$.
In this experiment, we fit Gaussian mixture models with spherical covariance matrices to the data, and apply each of the BIC, ICL, and merging clustering techniques.

Figure \ref{Example3} displays the data in (a) along with the BIC, ICL, and the merging clustering solutions (excluding $K = 1$).
For illustrative purposes, we show the results when using \texttt{MixMod} for the initialization parameters, as \texttt{mclust} yields the same clusterings for the BIC and ICL (corresponding to $K = 2$).
The BIC estimates $K = 5$, a clear overestimate since we know there are two true generating components, whereas the ICL does recover the correct group structure with $K = 2$.
The merging procedure iteratively merges the four components corresponding to the Gaussian group until it is represented as one cluster.
It is competitive with the ICL clustering, exhibiting a similar misclassification rate. 
Figure \ref{Entropy3} displays entropy plots for this example.
Interestingly, each plot suggests a change point of $K = 3$, although the presence of only five $K$-values might constitute an unreliable sample size.

  
\begin{figure}
\begin{center}
\subfloat[Unclustered data]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3_B/unclustered.pdf}
}
\subfloat[BIC solution, $K = 5$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3_B/merged_5_contour.pdf}
}
\subfloat[ICL solution, $K = 2$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3_B/ICL2_contour.pdf}
}
\\ \vspace{-1em}
\subfloat[Merged solution, $K = 4$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3_B/merged_4_contour.pdf}
}
\subfloat[Merged solution, $K = 3$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3_B/merged_3_contour.pdf}
}
\subfloat[Merged solution, $K = 2$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d3_B/merged_2_contour.pdf}
}
\end{center}
\caption{Unclustered data as well as labelled data with contours for each solution in example \ref{three}.}
\label{Example3}
\end{figure}

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d3_B/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d3_B/diff_entropy.pdf}
}
\\ \vspace{-1em}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d3_B/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.5\linewidth]{../Olson/Figures/d3_B/norm_diff.pdf}
}
\end{center}
\caption{Entropy plots for example \ref{three}.}
\label{Entropy3}
\end{figure}


\subsection{Comparison with Li's method using a Gaussian mixture}\label{four}
The next two examples compare the merging procedure of \cite{Baudry10} to a similar method laid out by \cite{Li05}, who also model clusters as Gaussian mixtures.
However, \cite{Li05} assumes the true number of clusters $K$ is assumed to be known in advance.
The algorithm is initialized using a two-layer $k$-means technique: one run of $k$-means for the $K$ clusters, and then a further $k$-means run within each cluster. 
The algorithm then merges clusters to minimize an inertia, or sum-of-squares, criterion; this contrasts with the entropy criterion of \cite{Baudry10}.
The total number of components is ultimately selected using the BIC.

We simulate from a four-component bivariate Gaussian mixture and apply our merging procedure.
The data are assumed to comprise three groups, with the two large rightmost components constituting a single group.
Thus, the ground truth for Li's method corresponds to $K = 3$.
Figure \ref{Example4_1} shows the unlabeled data as well as the BIC solution with $K = 4$ and the $K = 3$ merged solution.
We see that our merging routine identifies the desired group structure when merging to $K = 3$.
Conversely, Li's $K = 3$ solution merges the bottom two clusters (colored as cyan and blue), violating the desired structure.


\begin{figure}\label{Example4_1}
\begin{center}
\subfloat[Unclustered data]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d41/unclustered.pdf}
}
\subfloat[BIC solution, $K = 4$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d41/merged_4_contour.pdf}
}
\subfloat[Merged solution, $K = 3$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d41/merged_3_contour.pdf}
}
\end{center}
\caption{Unclustered data along with BIC and $K = 3$ merged solution for example \ref{four}.}
\end{figure}

\subsection{Comparison with Li's method using a trivariate uniform mixture with overlapping components}\label{five}
The second comparison with the method of \cite{Li05} uses a trivariate uniform mixture composed of two distinct groups: a horizontally-oriented ``cross'' and a vertically-oriented ``pillar''.
Figure \ref{Example4_2} provides both a 3-dimensional visualization (a) and a 2-dimensional projection (b) of the data.
We fit general Gaussian mixture models and apply our merging technique.
Here, we assume that the cross and pillar comprise two distinct groups, so that the ground truth for Li's method is $K = 2$. 

 \begin{figure}
\begin{center}
\subfloat[Unclustered data (3D view)]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_1_scatter.pdf}
}
\subfloat[Unclustered data (2D projection)]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_1_top_2d.pdf}
}
\\
\subfloat[BIC solution, $K = 3$ (3D view)]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_3_scatter.pdf}
}
\subfloat[BIC solution, $K = 3$ (2D projection)]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_3_top_2d.pdf}
}
\\
\subfloat[Merged solution, $K = 2$ (3D view)]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_2_scatter.pdf}
}
\subfloat[Merged solution, $K = 2$ (2D projection)]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d42/merged_2_top_2d.pdf}
}
\end{center}
\caption{Unclustered data as well as the BIC and merged clusterings for example \ref{five}.}
\label{Example4_2}
\end{figure}

The BIC chooses $K = 3$ components, each corresponding to a horizontal or diagonal pillar, with the labeled solution shown in figure \ref{Example4_2} (c -- d).
Applying the merging solution gives the $K = 2$ clustering that matches our notion of the true groups, with the labeled solution shown in figure \ref{Example4_2} (e -- f).
In contrast, Li's merging solution for $K = 2$ yields a clustering where the two legs of the cross closest to the vertical pillar are merged with the vertical pillar to form a group, leaving the other two legs of the cross as the second group.
This solution does not correspond to the assumed ground truth, with the clusters not being particularly well-separated.

\cite{Baudry10} contend that their method outperforms that of \cite{Li05}.
Clearly, sidestepping the need to know $K$ beforehand provides a broader range of candidate applications.
Their method also excels at separating clusters, whereas the method of \cite{Li05}, based on absolute distances between components, seems prone to yielding irrelevant merges. 
In addition, \cite{Li05} requires multiple runs of the EM algorithm, whereas \cite{Baudry10} gain efficiency by requiring only one.

\subsection{Flow cytometry study}\label{GvHD}
Our final example examines datasets from \cite{Brinkman07}.
These data comprise samples from a flow cytometry experiment in which white blood cell readings were collected from patients with and without graft-versus-host disease (GvHD).
GvHD occurs in stem cell transplant recipients when cells in the transplanted tissue attack several of the recipient's organs and tissues including the skin, gut, and liver.
This disease presents perhaps the biggest medical challenge in the field of tissue transplants.
Thus, it behooves us to ask: can we screen for GvHD susceptibility well before the procedure to prevent or mitigate its effects?

To address this question, \cite{Brinkman07} looked at a multitude of biomarker readings from 31 patients undergoing tissue transplants; 21 patients were eventually diagnosed with GvHD, 3 were determined not to have developed GvHD, and the remaining patients were inconclusive for various reasons.
They identified a significant correlation of GvHD susceptibility with the presence of cell subpopulations containing high levels of CD3, CD4, and CD8$\beta$ biomarkers, hereby referred to as CD3$^+$ CD4$^+$ CD8$\beta^+$ cell populations.
Furthermore, they used a manual gating analysis to group the measurements of CD3, CD4, CD8, and CD8$\beta$ of thousands of cells in each patient into cell subpopulations using physical and chemical properties.
The gating included CD8 on top of the three biomarkers of interest as these four biomarkers constitute all of the T cell helper/suppressor cells used in the experiment, whose collective expression levels determine the underlying biological populations.
The gating analysis identified six CD3$^+$ cell subpopulations in most GvHD-positive patients , three of which were also 
CD3$^+$ CD4$^+$ CD8$\beta^+$ cell subpopulations

\cite{Baudry10} look at two datasets from the study, corresponding to one ``positive'' patient who eventually developed GvHD and one ``control'' patient who did not.
The goal is to cluster the data into cell subpopulations using the BIC, ICL, and merging methods, and compare the results to those of \cite{Brinkman07}.
They assume a Gaussian mixture model with general covariances and apply each of the clustering techniques.
Following the strategy of \cite{Lo08}, a cell subpopulation cluster $c$ is considered positive in a biomarker $B$ if the estimated mean of the component is greater than 280 units in the $B$ dimension.
That is, if we estimate $\estim\mu_c = \left(\estim \mu_{c, \text{CD3}}, \estim \mu_{c, \text{CD4}}, \estim\mu_{c, \text{CD8}}, \estim\mu_{c, \text{CD8$\beta$}}\right) \in \reals^4$, then $c$ is considered CD3$^+$, for example, if $\estim\mu_{c, \text{CD3}} > 280$.
We can apply this threshold for each of the 4 biomarkers, and for each cluster.
To visualize CD3$^+$ CD4$^+$ CD8$\beta^+$ clusters, we can plot the labeled CD4 vs CD8$\beta$ readings, with horizontal and vertical lines corresponding to the 280 threshold, and then only display CD3$^+$ groups, i.e. clusters with $\estim\mu_{c, \text{CD3}} > 280$.
This strategy allows us to capture CD3$^+$ CD4$^+$ CD8$\beta^+$ populations as those which appear in the upper-right box of the CD4 vs CD8$\beta$ plot.
Note that $\estim\mu_c$ can be explicitly computed via \eqref{mixturemean}.

\begin{figure}
\begin{center}
\subfloat[BIC solution, $K = 16$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d51_raw_2/merged_15_CD3_CD8beta.pdf}
}
\subfloat[ICL solution, $K = 7$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d51_raw/ICL10_CD3_CD8beta.pdf}
}
\subfloat[Merged solution, $K = 8$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d51_raw_2/merged_8_CD3_CD8beta.pdf}
}
\end{center}
\caption{Clusterings of cell readings form a GvHD-positive patient using BIC, ICL, and the merging procedure in example \ref{GvHD}. The CD4 and CD8$\beta$ dimensions are shown, with only CD3$^+$ groups displayed. The CD8 dimension is not shown.}
\label{positive}
\end{figure}

We first examine the ``positive'' dataset from the patient who developed GvHD post-surgery.
The BIC solution in figure \ref{positive} (a) chooses $K = 16$ components, six of which are deemed CD3$^+$, and three of which are further deemed CD3$^+$ CD4$^+$ CD8$\beta^+$.
This result is consistent with the results of \cite{Brinkman07}.
The ICL solution in figure \ref{positive} (b) chooses $K = 7$ and identifies three CD3$^+$ groups, only one of which is CD3$^+$ CD4$^+$ CD8$\beta^+$ (plotted with black points).
The merged solution retains the same group structure as the BIC solution until $K = \estim K_\text{ICL} = 7$, when the bottom right cluster becomes CD3$^-$ and is no longer displayed (as in the ICL solution).
We display the merged solution for $K = 8$ in figure \ref{positive} (c), as this is the last solution in the decreasing sequence which exhibits the biologically consistent cluster structure in the BIC solution.
It also possesses significantly smaller entropy (1067 units) than both the BIC (6750 units) and ICL (3382 units) solutions.
We can make more sense of the clustering sequence by examining the entropy plots in figure \ref{Entropy5_1}.
\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d51_raw_2/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d51_raw_2/diff_entropy.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d51_raw_2/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d51_raw_2/norm_diff.pdf}
}
\end{center}
\caption{Entropy plots for the GvHD-positive patient in example \ref{GvHD}.}
\label{Entropy5_1}
\end{figure}
All four of the plots lead to a change point of $K = 9$.
Thus, we conclude that either choice of $K = 8$ or $K = 9$ is an appropriate selection for the dataset, and match the biological expectations given by \cite{Brinkman07}.

Next we examine the ``control'' dataset from the patient who did not develop GvHD.
The data and clusterings from each technique are shown in figure \ref{control}.
The BIC chooses $K = 11$, with three clusters considered to be CD3$^+$, one of which is also CD3$^+$ CD4$^+$ CD8$\beta^+$ (the black cluster in figure \ref{control} (a)).
This is inconsistent with the results of \cite{Brinkman07}, since the control patient should not have any such cell subpopulations.
The ICL chooses $K = 4$, with one cluster spanning much of the CD4-CD8$\beta$ plane (the black cluster in figure \ref{control} (b)).
It is more difficult to discern from the plot whether this is CD3$^+$ CD4$^+$ CD8$\beta^+$, and in fact, \cite{Baudry10} found an ICL solution ($K = 7$) which did contain a CD3$^+$ CD4$^+$ CD8$\beta^+$ population.
Hence, our ICL result should be considered inconclusive at best.
The merged solution for $K = 4$ displays two CD3$^+$ groups like the ICL, but zero CD3$^+$ CD4$^+$ CD8$\beta^+$ populations, which matches the biological expectations from \cite{Brinkman07}.
It also contains much lower entropy (145 units) than the BIC (4330 units) and ICL (313 units) solutions.
Entropy plots are shown in figure \ref{Entropy5_2}.
The first plot selects $K = 6$ as a change point, whereas the other three plots seem to select $K = 5$ as a change point.
From these plots as well as the ICL solution, $K = 4$, 5, or 6 comprise reasonable candidates for the merged solution, all of which are qualitatively similar and match the results of \cite{Brinkman07}.

\begin{figure}
\begin{center}
\subfloat[BIC solution, $K = 13$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d52_raw_2/merged_11_CD3_CD8beta.pdf}
}
\subfloat[ICL solution, $K = 4$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d52_raw_2/ICL_CD3_CD8beta.pdf}
}
\subfloat[Merged solution, $K = 4$]{
	\includegraphics[width=0.33\linewidth]{../Olson/Figures/d52_raw_2/merged_4_CD3_CD8beta.pdf}
}
\end{center}
\caption{Clusterings of cell readings form a GvHD-negative (control) patient using BIC, ICL, and the merging procedure in example \ref{GvHD}. The CD4 and CD8$\beta$ dimensions are shown, with only CD3$^+$ groups displayed. The CD8 dimension is not shown.}
\label{positive}
\label{control}
\end{figure}

\begin{figure}
\begin{center}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d52_raw_2/total_entropy.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d52_raw_2/diff_entropy.pdf}
}
\\
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d52_raw_2/cum_count.pdf}
}
\subfloat[]{
	\includegraphics[width=0.4\linewidth]{../Olson/Figures/d52_raw_2/norm_diff.pdf}
}
\end{center}
\caption{Entropy plots for the GvHD-negative (control) patient in example \ref{GvHD}.}
\label{Entropy5_2}
\end{figure}

\section{Discussion}
\cite{Baudry10} exploit the strengths of the BIC and ICL in a novel clustering procedure which merges components using an entropy criterion.
While their method draws on techniques from model-based clustering, its strengths lie in transgressing the assumption that each component corresponds to one cluster.
Instead, merging individual component densities into submixture densities themselves allows the number of clusters to be strictly smaller than the estimated number of mixture components.
In this light, the approach of using merged components to represent clusters can be viewed as a density-based clustering technique, in which clusters are defined as areas of high density for some underlying density \citep{Hans11}.
While the concept of merging components for clustering had already been established \citep{Hennig10}, the approach of \cite{Baudry10} has since been applied with success to several problems across multiple scientific disciplines \citep{Huang14, Ho12, Gormley11}. 

We have implemented the procedure and assessed its performance on several simulated examples along with an example using flow cytometry data.
It boasts competitive performance to the BIC and ICL methods, with particularly robust performance under model misspecification.
Moreover, outputting a sequence of clusterings, rather than committing to a sole solution, allows the modeler to choose the result that best suits their situation.
For example, a cross-cluster can be viewed as two overlapping groups or one contiguous group; this can possibly be resolved using the sequence of merged clusterings equipped with scientific context.
In addition, while the ICL attempts to incorporate both the likelihood fit and entropy into the clustering by packing them into a single criterion, the approach of \cite{Baudry10} better harnesses their individual contributions by incorporating them sequentially.
This yielded a significantly lower entropy with respect to a given $K$ for each example we considered, compared to both the BIC and ICL approaches.


Nonetheless, the procedure does come with caveats.
It begins with the BIC solution, which depends on a particular implementation of the EM algorithm (including tolerance and convergence criteria), which itself depends on a particular initialization scheme (which may or may not be deterministic).
Hence, the procedure is highly sensitive to the initialization woes that accompany computing $\estim K_\text{BIC}$.
Indeed, in most of our examples, the EM results from the two popular clustering packages, \texttt{mclust} and \texttt{MixMod}, yield different initial clusterings based on their respective initialization schemes.
While this often led to qualitatively similar clustering sequences in our examples, especially as $K$ continues to decrease, different initializations sometimes lead to very different conclusions.
Thus, we suggest performing a sensitivity analysis in practice to assess the effects of the initialization, guided by likelihood and entropy values if needed.

Furthermore, choosing $K$ from the sequence remains an unresolved problem with no satisfactory solution in sight.
We have seen examples where simply using $\estim K_\text{ICL}$ is unreliable, as well as examples where inferred change points are indecisive.
In addition, as the dimensionality of the data increases, it becomes very difficult to use visual or qualitative inspections of each clustering in the sequence to make a decision.
Possible approaches to this problem include more sophisticated change point detection techniques, or invoking a posterior distribution over the clustering sequence, but specific directions remain unclear.

 

\bibliography{stat572}

\section*{Appendix A: Theoretical foundations for ICL}
Assuming independence of $\bx_1, \dotsc, \bx_n$, by \eqref{density} the observed likelihood is
\ba
f(\bx | K, \theta_K)
	& = \prod_{i=1}^n \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k).
\ea
Taking the logarithm yields the observed log-likelihood
\begin{equation}\label{l}
\begin{split}
\ell(\theta_K | \bx, K)
	& = \log\left( \prod_{i=1}^n \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right) \\
	& = \sum_{i=1}^n \log \left( \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right).
\end{split}
\end{equation}
Now, noting that for a unit vector $\bz_i \in \reals^K$ whose $k$th element is one, 
\ba
\Pr{\bz_i} = \Pr{z_{ik} = 1} = p_k,
\ea
yielding the complete likelihood for one observation to be
\ba
f(\bx_i, \bz_i)
	& = f(\bx_i | \bz_i) \Pr{\bz_i} \\
	& = \sum_{k=1}^K \phi(\bx_i | \mu_k, \Sigma_k) p_k \ind{\bz_{ik} = 1}
		 \\
	& = \prod_{k = 1}^K 
		\left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}}.
\ea
Hence, the complete likelihood is
\ba
f(\bx, \bz| K, \theta_K)
	& = \prod_{i=1}^n \prod_{k = 1}^K 
		\left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}} \\
\ea
as stated in \eqref{completedensity}.
Taking the logarithm yields the complete log-likelihood
\begin{equation}\label{lc}
\begin{split}
\ell_\text{C}(\theta_K | \bx, \bz, K)
	& = \log\left(\prod_{i=1}^n \prod_{k = 1}^K 
		\left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}} \right)\\
	& = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \log \left( p_k \phi(\bx_i | \mu_k, \Sigma_k) \right).
\end{split}
\end{equation}
Now, let $t_{ik}$ be the conditional probability that $\bx_i$ comes from component $k$.
Then from Bayes's theorem, for a Gaussian mixture, we have
\begin{align}\label{tik}
t_{ik} \nonumber
	& = t_{ik}(K, \btheta_K) \\  \nonumber
	& = \Pr{\text{component } k \mid \bx_i, K, \btheta_K} \\
	& = \frac{\Pr{ \text{component }k, \bx_i \mid K, \btheta_K}}
		{f(\bx_i|K,\btheta_K)} \nonumber \\ 
	& = \frac{\Pr{ \bx_i | \text{component } k, K, \btheta_K }
		\Pr{\text{component } k | K, \btheta_K}}
		{ \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) } \nonumber \\
 	& = \frac{ \phi(\bx_i | \mu_k, \Sigma_k) \cdot p_k }
 	{ \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) }.
\end{align}
Defining
\ba
\text{EC}(K | \bz) := - \sum_{k=1}^K \sum_{i=1}^n z_{ik} \log t_{ik},
\ea
we see that
\ba
\; & \ell(\theta_K|\bx, \bz) - \text{EC}(K|\bz) \\
	& = \sum_{i=1}^n \log\left( 
		\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		- \left( - \sum_{k=1}^K \sum_{i=1}^n z_{ik} \log(t_{ik}) \right)
		\htab \text{(by \eqref{l})} \\
	& = \sum_{i=1}^n \left\{
		\log\left(\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		+ \sum_{k=1}^K z_{ik} \log\left( \frac{p_k \phi(\bx_i | \mu_k, \Sigma_k)}{ \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) } \right)
		\right\} \\
	& = \sum_{i=1}^n \Bigg\{
		\log\left(\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		\\
	& \hspace{3em} + \sum_{k=1}^K z_{ik} \left[ \log\left( p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
		- \log \left( \sum_{j=1}^K p_j \phi(\bx_i | \mu_j, \Sigma_j) \right) \right]
		\Bigg\} \\
	& = \sum_{i=1}^n \sum_{k=1}^K z_{ik} \log(p_k \phi(\bx_i | \mu_k, \Sigma_k))
		+ \sum_{i=1}^n \log\left(\sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \right)
			\underbrace{\left[ 1 - \sum_{i=1}^K z_{ik} \right]}_{=0} \\
	& = \sum_{i=1}^n \sum_{k=1}^K z_{ik} 
		\log(p_k \; \phi(\bx_i | \mu_k, \Sigma_k)) \\
	& \equiv \ell_\text{C}(\theta_K|\bx, \bz) \htab \text{(by \eqref{lc})}.
\ea
Viewing EC$(K | \bZ)$ as a random variable in $\bZ$, we have for fixed $K$,
\ba
\E{ \text{EC}(K | \bZ) } & = - \sum_{k=1}^K \sum_{i=1}^n \E{Z_{ik}} \log t_{ik} \\
	& = - \sum_{k=1}^K \sum_{i=1}^n \Pr{Z_{ik} = 1} \log t_{ik} \\
	& = - \sum_{k=1}^K \sum_{i=1}^n \Pr{X_i \text{ comes from $k$th component} } \log t_{ik} \\
	& \equiv -  \sum_{k=1}^K \sum_{i=1}^n t_{ik} \log t_{ik} \\
	& = \text{Ent}(K),
\ea
the entropy of the matrix $\bT = \set{t_{ik}}_{1\le i \le n; 1 \le k \le K}$.
In other words, $\text{EC}(K | \bZ)$ is a random variable whose mean is the classification entropy.

Now, the integrated likelihood (aka evidence or model evidence) is
\ba
\text{IL}(\bx | K, \theta_K)
	& = \int_{\Theta_{K}} f(\bx | K, \theta_K) \pi(\theta_K | K) \; \der \theta_K \\
	& = \int_{\Theta_{K}} \prod_{i=1}^n \sum_{k=1}^K p_k \phi(\bx_i | \mu_k, \Sigma_k) \pi(\theta_K | K) \; \der \theta_K
\ea
An approximation is given via
\ba
	\log \text{IL}(\bx | K, \theta_K) 
		& \approx \log f\left(\bx | K, \estim\theta_{K}\right)
		 - \frac{\nu_K \log(n)}{2} \\
		& \equiv \text{BIC}\left(K|\bx, \estim\theta_K \right)
\ea
where $\estim\theta_K = \estim\theta_{K, \text{MLE}} = \argmax_\theta f(\bx | K, \theta)$. 
Analogously, we define the integrated complete likelihood as
\ba
\text{ICL}(\bx, \bz | K)
	& = \int_{\Theta_K} f(\bx, \bz | K, \theta_K) \pi(\theta_K | K) \; \der \theta_K \\
	& = \int_{\Theta_K} \prod_{i=1}^n \prod_{k = 1}^K 
		\left[ p_k \phi(\bx_i | \mu_k, \Sigma_k) \right]^{z_{ik}} \pi(\theta_K | K) \; \der \theta_K \\
\ea
which takes into account evidence for an effective clustering structure. 
By setting
\begin{equation}\label{map}
\estim z_{ik} = \ind{ \argmax_{1 \le \ell \le K} t_{i \ell}\left(\estim\theta_{K}\right) = k }
\end{equation}
for $1 \le i \le n$ and $1 \le k \le K$,
we introduce the ICL criterion as a similar approximation to BIC using our ``completed'' data $(\bx, \estim\bz)$:
\ba
 \estim{\text{ICL}}\left(K|\bx, \estim\bz, \estim\theta_K\right) 
	& \approx 
	\log f\left(\bx, \estim\bz | K, \estim\theta_{K, \text{MLE}}\right) - 
		\frac{ \nu_K \log(n) }{2} \\
	& = \ell \left(\theta_K | \bx, \estim\bz \right) 
		- \text{EC}\left(K|\estim\bz\right) 
		- \frac{ \nu_K \log(n) }{2} \\
	& = \text{BIC}(K|\bx) - \text{EC}\left(K|\estim\bz\right).
\ea
Thus, it is seen that the ICL criterion is a penalization of the BIC by a quantity whose mean is the classification entropy.

\section*{Appendix B: Details of merging procedure criterion}
Suppose we merge clusters $k$ and $k'$ to form $k \cup k'$. Then by mutual exclusivity,
\ba
t_{i, k \cup k'}^K
	& = \Pr{\bx_i \text{ comes from $(k \cup k')$th component}} \\
	& = \Pr{\bx_i \text{ comes from $k$th or $k'$th component} } \\
	& = \Pr{\bx_i \text{ comes from $k$th component }} + \Pr{\bx_i \text{ comes from $k'$th component} } \\
	& = t_{ik}^K + t_{ik'}^K.
\ea
In other words, for a component $c$ that is a Gaussian mixture, the conditional probability that $c$ generated $\bx_i$ is the combined probability that any of its subcomponents $c_j$ generated $\bx_i$.
The resultant entropy (after relabeling the indices as necessary) is
\ba
\text{Ent}(K - 1|k, k') & = - \sum_{k=1}^{K - 1} \sum_{i=1}^n t_{ik} \log t_{ik} \\
	& = - \sum_{i=1}^n \left( \sum_{j \ne k \cup k'} t_{ij} \log(t_{ij}) + t_{i,k \cup k'} \log( t_{i,k\cup k'} ) \right) \\
	& = - \sum_{i=1}^n \left( \sum_{j \ne k \cup k'} t_{ij} \log(t_{ij}) + (t_{i,k}^K + t_{i,k'}^K) \log( t_{ik}^K + t_{ik'}^K) \right).
\ea
The corresponding difference in entropy is
\ba
\; & \Delta\text{Ent}(k, k' | K) \\
	& \equiv \text{Ent}(K) - \text{Ent}(K - 1|k, k') \\
	& = -  \sum_{i=1}^n \sum_{j=1}^K t_{ij} \log t_{ij} + \sum_{i=1}^n \sum_{j=1}^{K - 1} t_{ij} \log t_{ij} \\
	& = - \sum_{i=1}^n \left\{
			\sum_{j \ne k, k'} t_{ij} \log t_{ij}
			+ t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'}
			+ \sum_{j \ne k \cup k'} t_{ij} \log t_{ij}
			+ t_{i, k\cup k'} \log t_{i, k \cup k'}
		\right\} \\
	& = - \sum_{i=1}^n \left\{ t_{ik} \log t_{ik} + t_{ik'} \log t_{ik'} \right\}
		+ \sum_{i=1}^n t_{i,k\cup k'} \log t_{i, k \cup k'}
\ea
leading to the criterion in \eqref{delta_ent}.



\section*{Appendix D: Mean of a Gaussian submixture}
A cluster is considered positive in a biomarker $B$ if its estimated mean $\mu_B$ is $\ge 280$ units.
In order to compute its mean, we need to derive the expression for a mean of a submixture of Gaussians, which can be done via the following.
Let $f_s$ be a density corresponding to a submixture of $m \le K$ Gaussian component densities.
Then without loss of generality we can write
\ba
f_s(\bx) = \sum_{j=1}^m p_j \phi_j(\bx | \mu_j, \Sigma_j)
\ea
(otherwise, just relabel the clusters).
Let $X \sim f_s$ be a sample from this density, i.e., a sample from the overall mixture conditioned on the component belonging to this particular submixture.
Let $J$ be the generating component of $X$.
Then
\ba
J \sim \text{Multinomial}\left(m, \frac{p_1}{\sum_{k=1}^m p_k}, \dotsc,
\frac{p_K}{\sum_{k=1}^m p_k}\right).
\ea
Therefore, by the law of total expectation,
\begin{align}\label{mixturemean}
\E{X}
	& = \sum_{j=1}^m \E{X \mid J = j }
			\Pr{ J = j } \nonumber \\
	& = \sum_{j=1}^m \mu_j \cdot \frac{p_j}{ \sum_k^m p_k } \nonumber \\
	& = \frac{ \sum_{j=1}^m p_j \mu_j }{ \sum_{j=1}^m p_j }
\end{align}
which verifies the claim.

\section*{Appendix E: Entropy tables}
\newpage
\begin{table}
\input{../Olson/Figures/d1/K_ent.tex} 
\caption{Entropy values for example 1}
\label{Table1}
\end{table}

\begin{table}
\input{../Olson/Figures/d2/K_ent.tex}
\caption{Entropy values for example 2}
\label{Table2}
\end{table}

\begin{table}
\input{../Olson/Figures/d3_B/K_ent.tex}
\caption{Entropy values for example 3}
\label{Table3}
\end{table}

\begin{table}
\input{../Olson/Figures/d41/K_ent.tex}
\caption{Entropy values for example 4.1}
\label{Table41}
\end{table}

\begin{table}
\input{../Olson/Figures/d42/K_ent.tex}
\caption{Entropy values for example 4.2}
\label{Table42}
\end{table}

\begin{table}
\input{../Olson/Figures/d51/K_ent.tex}
\caption{Entropy values for example 5.1}
\label{Table51}
\end{table}

\begin{table}
\input{../Olson/Figures/d52/K_ent.tex}
\caption{Entropy values for example 5.2}
\label{Table52}
\end{table}

\end{document}









